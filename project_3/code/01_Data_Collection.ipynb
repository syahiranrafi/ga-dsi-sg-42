{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use, make a [Reddit](https://www.reddit.com/register/) account and sign up to obtain the [API](https://www.reddit.com/wiki/api/) key. \n",
    "\n",
    "Insert **client_id**, **client_secret** & **user_agent** from Reddit after obtaining your API key in the code text below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input your API key details here\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"4TckO1aMiQwyp1mfysQhrA\", #input your client_id here\n",
    "    client_secret=\"VeqTyxzfRidxh1Za1Rc-u4YV3TyZfw\", #input your client_secret here\n",
    "    user_agent=\"WebScraper\", #input your user_agent here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function below responsible for all scraping from Reddit\n",
    "\n",
    "Function scrapes both posts and comments from the top section of specified subreddit.\n",
    "\n",
    "The reason behind scraping from top specifically is to ensure dataset is not biased towards current events and is able to gather well-populated comments in posts from different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_posts_and_comments_from_reddit_top(subreddit_name, num_post, time_period, comments_per_post):\n",
    "    # List to store scraped comments\n",
    "    scraped_posts = []\n",
    "    scraped_comments = []\n",
    "    start_time = time.time() #Timestamp for progress bar\n",
    "    total_posts = num_post*comments_per_post\n",
    "    # Function to scrape both post and comments from a subreddit\n",
    "    def scrape(subreddit_name, num_post, time_period):\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        num_scraped = 0 # num_scraped represents total number of posts scraped so far\n",
    "        comment_counter = 0 # comment_counter represents total comments scraped, for progress bar\n",
    "        last_post_time = None  # Initialize last_post_time here\n",
    "        for submission in subreddit.top(time_filter=time_period, limit=None, params={'after': last_post_time}):\n",
    "            if num_scraped >= num_post: # Breaks for loop once desired post count reached\n",
    "                break\n",
    "            comment_per_post_counter = 0 #comment_per_post_counter represents comments scraped for a single post, to ensure comments do not exceed comments_per_post\n",
    "            scraped_posts.append([submission.id, submission.title, submission.score, submission.num_comments, submission.selftext, submission.created_utc, submission.upvote_ratio, subreddit_name])\n",
    "            submission.comments.replace_more(limit=10) # Limit set to None to ensure all comments are seen. If 429 Error, change to limit = 10 to resolve it\n",
    "            for comment in submission.comments.list():\n",
    "                scraped_comments.append([comment.id, comment.parent_id, comment.link_id, comment.is_submitter, comment.body, comment.score, comment.stickied, comment.created_utc, submission.title, subreddit_name])\n",
    "                time.sleep(1) # Time delay function to prevent HTTP 429 Error\n",
    "                if comment_counter % int(total_posts/10) == 0: #Progress bar prints absolute value of comments scraped based on 10% of the total desired comments determined by total_posts\n",
    "                    comment_time = time.time() # Timestamp for progress bar\n",
    "                    print(f'{comment_counter} comments scraped in {round((comment_time-start_time)/60,2)} minutes')\n",
    "                comment_per_post_counter += 1 \n",
    "                comment_counter += 1\n",
    "                if comment_per_post_counter >= comments_per_post:\n",
    "                    break\n",
    "            num_scraped += 1\n",
    "            last_post_time = submission.created_utc #Timestamp for progress bar\n",
    "            time.sleep(1)  # Time delay function to prevent HTTP 429 Error\n",
    "    \n",
    "    # Scrape comments from each subreddit\n",
    "    scrape(subreddit_name, num_post, time_period)\n",
    "    \n",
    "    # Convert scraped comments to DataFrame\n",
    "    posts_columns = ['id', 'title', 'score', 'num_comments', 'selftext', 'created_utc', 'upvote_ratio', 'subreddit']\n",
    "    comments_columns = ['comment_id', 'parent_id', 'post_id', 'is_submitter', 'body', 'score', 'stickied', 'created_utc', 'post_title', 'subreddit']\n",
    "    df_posts = pd.DataFrame(scraped_posts, columns=posts_columns)\n",
    "    df_comments = pd.DataFrame(scraped_comments, columns=comments_columns)\n",
    "    \n",
    "    # Convert 'created_utc' column to datetime\n",
    "    df_posts['created_utc'] = pd.to_datetime(df_posts['created_utc'], unit='s')\n",
    "    df_comments['created_utc'] = pd.to_datetime(df_comments['created_utc'], unit='s')\n",
    "\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Done in {round((end_time-start_time)/60,2)} minutes\")\n",
    "    return(df_posts, df_comments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping for r/TheOnion and r/news\n",
    "\n",
    "200 posts of 100 comments each to ensure diverse set of comments and prevent overfitting on a small set of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 comments scraped in 2.37 minutes\n",
      "20 comments scraped in 7.06 minutes\n",
      "40 comments scraped in 9.34 minutes\n",
      "60 comments scraped in 9.79 minutes\n",
      "80 comments scraped in 10.24 minutes\n",
      "100 comments scraped in 13.87 minutes\n",
      "120 comments scraped in 14.35 minutes\n",
      "140 comments scraped in 14.76 minutes\n",
      "160 comments scraped in 15.56 minutes\n",
      "180 comments scraped in 16.05 minutes\n",
      "Done in 16.41 minutes\n"
     ]
    }
   ],
   "source": [
    "subreddit_name = \"TheOnion\"\n",
    "\n",
    "theonion_df_posts, theonion_df_comments = scrape_posts_and_comments_from_reddit_top(subreddit_name= subreddit_name, num_post=20,time_period= \"all\", comments_per_post=10)\n",
    "\n",
    "theonion_df_comments.to_csv('../data/01_raw_onion_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 comments scraped in 0.35 minutes\n",
      "20 comments scraped in 1.36 minutes\n",
      "40 comments scraped in 2.14 minutes\n",
      "60 comments scraped in 3.04 minutes\n",
      "80 comments scraped in 3.88 minutes\n",
      "100 comments scraped in 4.7 minutes\n",
      "120 comments scraped in 5.5 minutes\n",
      "140 comments scraped in 6.34 minutes\n",
      "160 comments scraped in 7.26 minutes\n",
      "180 comments scraped in 8.17 minutes\n",
      "Done in 8.76 minutes\n"
     ]
    }
   ],
   "source": [
    "subreddit_name = \"news\"\n",
    "\n",
    "news_df_posts, news_df_comments = scrape_posts_and_comments_from_reddit_top(subreddit_name= subreddit_name, num_post=20,time_period= \"all\", comments_per_post=10)\n",
    "\n",
    "news_df_comments.to_csv('../data/01_raw_news_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - Print statements above are for illustration purposes to re-enact the scraping of the actual 20,000 (estimate) comments per subreddit from r/news and r/TheOnion. Actual scraped data_sets are labeled similiarly to the export function as written in code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posts and comments were scraped separately for faster processing in EDA. As the comments CSV file accumulated up to more than 35,000 rows, handling of the comments file took significantly longer processing time than handling of the posts file, which only had 100 rows. However, it was determined that the EDA for the post_only csv file was not meaningful due to too small a sample size of only a 100 posts. Therefore the post_only dataframe generated above will not be exported out as a csv file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
