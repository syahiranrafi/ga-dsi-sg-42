{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_Data Cleaning\n",
    "\n",
    "Data cleaning is responsible to clean up the raw data into cleaned data for EDA and modeling. \n",
    "\n",
    "Data cleaning process implemented include:\n",
    "1. remove URL, \n",
    "2. remove newline\n",
    "3. replace [deleted] and [removed] with \"pseudodeleted\" and \"pseudoremoved\"\n",
    "4. remove newline /carriage return \n",
    "5. remove stop words \n",
    "6. remove punctuation and special characters  # only execute after stop words removal, as \"wasn’t\" is a stop word, but not \"wasnt\" \n",
    "7. convert to lower → Lemmatization \n",
    "8. remove lines with null comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to install \"spacy\" library from pip, and download the \"en_core_web_sm\" language pack. \n",
    "\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to install textblob, a library to perform spelling check and correction\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to install pyspellchecker, a library to perform spelling check and correction\n",
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Initialize variables/objects that will be used for data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "special_char_list = list(string.punctuation)\n",
    "special_char_list+=[\"’\",\"'s\",\"’s\",\"...\",\"$\",\"@$$.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Functions to perform data cleaning\n",
    "1. lemmatization - using spacy (*evaluated, but eventually not used in the data cleaning*)\n",
    "2. lemmatization - using WordNetLemmatizer\n",
    "3. stemming (*evaluated, but eventually not used in the data cleaning*)\n",
    "4. text cleaning\n",
    "5. spell check - using spellchecker (*evaluated, but eventually not used in the data cleaning*)\n",
    "6. spell check - using TextBlob (*evaluated, but eventually not used in the data cleaning*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [unused function were deemed unnecessary for the final cleaning of dataset]. Spacy lemmatizer was unable to lemmatize continuous tense to its lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ delete ]\n",
      "[ remove ]\n",
      "I be pseudoremove\n",
      "I be pseudodelete ! !\n",
      "nan\n",
      "nan\n",
      "hellooooo ! !\n",
      "fucking fuck fucker fucking fuckin ! !\n",
      "be not ! !\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "function to perform lemmatization on a text (Not in use, but keep here for future reference)\n",
    "Spacy is a  library to perform lemmatization using a much more efficient algo for big data set. \n",
    "\n",
    "Afternote: Spacy was not used in the end, \n",
    "'''\n",
    "def sentence_lemmatizer_spacy(text):\n",
    "    if text.strip() == '':\n",
    "        return np.NaN\n",
    "\n",
    "    doc = nlp(text.lower())                                 # Process the text with SpaCy\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]     # Extract lemmas for each token\n",
    "    return ' '.join(lemmatized_tokens)  \n",
    "\n",
    "#unit test\n",
    "print(sentence_lemmatizer_spacy(\"[deleted]\"))   \n",
    "print(sentence_lemmatizer_spacy(\"[removed]\"))   \n",
    "print(sentence_lemmatizer_spacy(\"i am pseudoremoved\"))\n",
    "print(sentence_lemmatizer_spacy(\"i am pseudodeleted !!\")) \n",
    "print(sentence_lemmatizer_spacy(\" \"))   \n",
    "print(sentence_lemmatizer_spacy(\"\"))       \n",
    "print(sentence_lemmatizer_spacy(\"hellooooo !!\"))    \n",
    "print(sentence_lemmatizer_spacy(\"fucking fuck fucker FUCKING fuckin!!\"))          \n",
    "print(sentence_lemmatizer_spacy(\"wasn't !!\"))         # Join the lemmatized tokens back into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ delete ]\n",
      "[ remove ]\n",
      "i be pseudoremoved\n",
      "i be pseudodeleted ! !\n",
      "hellooooo ! !\n",
      "nan\n",
      "nan\n",
      "fuck fuck fucker fuck fuckin ! !\n",
      "be n't ! !\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "function to perform lemmatization on a text\n",
    "Using the WordNetLemmatizer by nltk, which is much slower\n",
    "'''\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def sentence_lemmatizer(text):\n",
    "    if text.strip() == '':\n",
    "        return np.NaN\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    char_list = word_tokenize(text.lower())\n",
    "\n",
    "    # Lemmatize list of words and join\n",
    "    return ' '.join([lemmatizer.lemmatize(w.lower(), get_wordnet_pos(w.lower())) for w in char_list])\n",
    "\n",
    "\n",
    "#unit test\n",
    "print(sentence_lemmatizer(\"[deleted]\"))   \n",
    "print(sentence_lemmatizer(\"[removed]\"))   \n",
    "print(sentence_lemmatizer(\"i am pseudoremoved\"))\n",
    "print(sentence_lemmatizer(\"i am pseudodeleted !!\"))     \n",
    "print(sentence_lemmatizer(\"hellooooo !!\"))             \n",
    "print(sentence_lemmatizer(\" \"))   \n",
    "print(sentence_lemmatizer(\"\"))   \n",
    "print(sentence_lemmatizer(\"fucking fuck fucker FUCKING fuckin!!\"))   \n",
    "print(sentence_lemmatizer(\"wasn't !!\")) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [unused function were deemed unnecessary for the final cleaning of dataset]. Stemmer is less effective than choosen lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ delet ]\n",
      "[ remov ]\n",
      "i am pseudoremov\n",
      "i am pseudodelet ! !\n",
      "hellooooo ! !\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function to do text stemming using PorterStemmer (Not in use, but keep here for future reference)\n",
    "'''\n",
    "def sentence_stemmer(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return ' '.join([porter_stemmer.stem(word) for word in words])\n",
    "\n",
    "\n",
    "#unit test\n",
    "print(sentence_stemmer(\"[deleted]\"))  \n",
    "print(sentence_stemmer(\"[removed]\"))    \n",
    "print(sentence_stemmer(\"i am pseudoremoved\"))\n",
    "print(sentence_stemmer(\"i am pseudodeleted !!\"))     \n",
    "print(sentence_stemmer(\"hellooooo !!\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc abcc abccc abcccc hellllo CY coffee cup cost  288 sugar belongs Ryan\n",
      "pseudodeleted\n",
      "pseudodeleted\n",
      "pseudoremoved\n",
      "pseudoremoved\n",
      "1 \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "5 \n",
      "6 URL\n",
      "7 \n",
      "8 \n",
      "9 OKOK Got 3rd line\n",
      "10 OMG\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Function to do basic cleaning of the comment text\n",
    "- remove newline\n",
    "- remove http url\n",
    "- remove [deleted], [removed] \n",
    "- (more to come... )\n",
    "'''\n",
    "def text_cleaning(text):\n",
    "\n",
    "    url_regex = re.compile(\n",
    "    r'((http|https)://'     # Start with http:// or https://\n",
    "    r'([a-zA-Z0-9.-]+)'     # Match the domain name (alphanumeric characters, dots, and dashes)\n",
    "    r'(\\.[a-zA-Z]{2,})'     # Match the top-level domain (e.g., .com, .net) with at least 2 characters\n",
    "    r'(:\\d+)?'              # Match an optional port number\n",
    "    r'(/\\S*)?'              # Match an optional path (any non-whitespace characters)\n",
    "    r'(\\?[^\"\\s]*)?)',        # Match an optional query string (attribute-value pairs)\n",
    "    re.IGNORECASE        # Ignore case sensitivity\n",
    "    )\n",
    "\n",
    "    # remove URL\n",
    "    text = url_regex.sub(\"\", text)\n",
    "\n",
    "    # Mark the comment with [deleted] or [removed] with pseudo marker \"pseudodeleted\" and \"pseudoremoved\"\n",
    "    # After lemmatization, [deleted] become [ delete ], [removed] become [ remove ]\n",
    "    # After stemming, [deleted] become [ delet ], [removed] become [ remov ]\n",
    "    text = str(text).replace(\"[deleted]\",\"pseudodeleted\").replace(\"[removed]\",\"pseudoremoved\").replace(\"[ delete ]\",\"pseudodeleted\").replace(\"[ remove ]\",\"pseudoremoved\").replace(\"[ delet ]\",\"pseudodeleted\").replace(\"[ remov ]\",\"pseudoremoved\")\n",
    "\n",
    "    # remove newline \n",
    "    text = str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\r\\n\",\" \").replace(\"_x000D_\", \" \")\n",
    "\n",
    "    # remove  \"'s\"\n",
    "    text = re.sub(r\"(\\'s)\",\"\", text)\n",
    "    \n",
    "    # remove stopword \n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "   # remove punctuation and special character\n",
    "    text = ''.join([char for char in text if char not in special_char_list])\n",
    "\n",
    "    # return the cleaned text\n",
    "    return text.strip()\n",
    "\n",
    "#unit test\n",
    "print(text_cleaning(\"abc abcc abccc abcccc hellllo CY's coffee!! A cup cost @ $2.88 and no sugar belongs to Ryan's\"))\n",
    "print(text_cleaning(\",[deleted],\"))\n",
    "print(text_cleaning(\",[ delete ],\"))\n",
    "print(text_cleaning(\",[ remove ],\"))\n",
    "print(text_cleaning(\",[removed],\"))\n",
    "print(\"1\",text_cleaning(\"http://www.google.com\"))\n",
    "print(\"2\",text_cleaning(\"http://www.google.com/\"))\n",
    "print(\"3\",text_cleaning(\"http://www.yahoo.com.sg:8601\"))\n",
    "print(\"4\",text_cleaning(\"http://www.yahoo.com.sg:8601/\"))\n",
    "print(\"5\",text_cleaning(\"http://www.yahoo.com.sg:8601/api\"))\n",
    "print(\"6\",text_cleaning(\"This is my URL http://www.YAhoo.com.sg:8601/api :)\"))\n",
    "print(\"7\",text_cleaning(\"https://www.galolawfirm.com:8888/how-to-legally-terminate-an-employee-in-texas/#:~:text=Texas%2C%20like%20many%20U.S.%20states,it's%20not%20an%20unlawful%20one.\"))\n",
    "print(\"8\",text_cleaning(\"https://graphics.stltoday.com/apps/payrolls/salaries_2020/teachers/?sort=med_salary&dir=desc\"))\n",
    "print(\"9\",text_cleaning(\"https://graphics.stltoday.com/apps/payrolls/salaries_2020/teachers/?sort=med_salary&dir=desc\\n\\\n",
    "                        OKOK Got it \\n \\\n",
    "                        This is 3rd line!!! \"))\n",
    "print(\"10\",text_cleaning(\"OMG!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [unused function were deemed unnecessary for the final cleaning of dataset]. TextBlob Spell check is ineffective with slang and some common words (see unit test output for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, this is of, I am tying out the spelling cheer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Plump is going to throw a massive hissyfit.\n",
      "Must anna say congress from the U.K. guns! He€™ve been just as nervous/excited as you! Today is a good day for America. I sincerely hope you all a healthy and prosperous future!\n"
     ]
    }
   ],
   "source": [
    "def perform_spell_check_correction_textblob(text):\n",
    "    tb = TextBlob(text)\n",
    "    return tb.correct()\n",
    "\n",
    "\n",
    "#unit test\n",
    "print(perform_spell_check_correction_textblob(\"helloo, this is CY, I am tying out teh speling cheker\"))\n",
    "print(perform_spell_check_correction_textblob(\"Donald Trump is going to throw a massive hissyfit.\"))\n",
    "print(perform_spell_check_correction_textblob(f\"Just wanna say congrats from the U.K. guys! Weâ€™ve been just as nervous/excited as you! Today is a good day for America. \\\n",
    "\\\n",
    "I sincerely hope you all a healthy and prosperous future!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [unused function were deemed unnecessary for the final cleaning of dataset]. Decided not to perform spell check as the computation was too expensive (estimated more than 20hour for the 37K data records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello this is CY, I am tying out the spelling cheer\n",
      "Donald Trump is going to throw a massive hissyfit.\n",
      "Just wanna say congrats from the U.K. guys Weâ€™ve been just as nervous/excited as you Today is a good day for America. I sincerely hope you all a healthy and prosperous future\n"
     ]
    }
   ],
   "source": [
    "def perform_spell_check_correction_spellchecker(text):\n",
    "    words = text.split()\n",
    "    spell = SpellChecker()\n",
    "    misspell = list(spell.unknown(words))\n",
    "    #print(f\"MISS=={misspell}\")\n",
    "    for word in misspell:\n",
    "        correct_spell = spell.correction(word)\n",
    "        if correct_spell != None: \n",
    "            text = text.replace(word, correct_spell)\n",
    "\n",
    "    #print(f\"FIXED>>> {text}\")\n",
    "    return text \n",
    "\n",
    "\n",
    "#unit test\n",
    "print(perform_spell_check_correction_spellchecker(\"helloo, this is CY, I am tying out teh speling cheker\"))\n",
    "print(perform_spell_check_correction_spellchecker(\"Donald Trump is going to throw a massive hissyfit.\"))\n",
    "print(perform_spell_check_correction_spellchecker(f\"Just wanna say congrats from the U.K. guys! Weâ€™ve been just as nervous/excited as you! Today is a good day for America. \\\n",
    "\\\n",
    "I sincerelly hope you all a healthy and prosporous future!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We decided to forego spellcheck (both TextBlob and SpellChecker) as we could not achieve balance of accuracy and efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Perform Text Cleaning\n",
    "- concatenate onion comments and news comments into single dataframe\n",
    "- check and drop comments which are blank, if any\n",
    "- verify column datatype\n",
    "- export the cleaned dataframe to a CSV , ready for next step - EDA and Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "onion_file = f\"../data/01_raw_onion_data.csv\"\n",
    "news_file = f\"../data/01_raw_news_data.csv\"\n",
    "\n",
    "onion_df = pd.read_csv(onion_file)\n",
    "news_df = pd.read_csv(news_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Concatenate the news file and onion file into the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>stickied</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>post_title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dpep775</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>False</td>\n",
       "      <td>I love how onion updates to The city</td>\n",
       "      <td>1349</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-06 02:49:19</td>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>TheOnion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dpee371</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>False</td>\n",
       "      <td>...again.</td>\n",
       "      <td>4780</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-05 23:14:31</td>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>TheOnion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dpex3x5</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>False</td>\n",
       "      <td>Just need to finish it off with \"Our thoughts ...</td>\n",
       "      <td>262</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-06 05:53:10</td>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>TheOnion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dpejj86</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>False</td>\n",
       "      <td>thousands of people going to prison for weed E...</td>\n",
       "      <td>2265</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-06 00:57:39</td>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>TheOnion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dpeiwa9</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>False</td>\n",
       "      <td>Every law abiding citizen in Australia that wa...</td>\n",
       "      <td>1958</td>\n",
       "      <td>False</td>\n",
       "      <td>2017-11-06 00:44:49</td>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>TheOnion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37914</th>\n",
       "      <td>gfv68hn</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>False</td>\n",
       "      <td>I predict this confirmation thing is going to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-14 23:40:41</td>\n",
       "      <td>President-elect Joe Biden clears 270-vote thre...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37915</th>\n",
       "      <td>gfv78uj</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>False</td>\n",
       "      <td>I'm excited for the cheeto's rage tweeting later</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-14 23:49:44</td>\n",
       "      <td>President-elect Joe Biden clears 270-vote thre...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37916</th>\n",
       "      <td>gfv7xr7</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>False</td>\n",
       "      <td>I wonder if Republicans will purposely not ref...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-14 23:56:06</td>\n",
       "      <td>President-elect Joe Biden clears 270-vote thre...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37917</th>\n",
       "      <td>gfv8r3z</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>False</td>\n",
       "      <td>Looking forward to when the whitehouse finally...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-15 00:03:35</td>\n",
       "      <td>President-elect Joe Biden clears 270-vote thre...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37918</th>\n",
       "      <td>gfv9npw</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>t3_kd8bdc</td>\n",
       "      <td>False</td>\n",
       "      <td>And the world takes a collective sigh of relief.</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-15 00:12:01</td>\n",
       "      <td>President-elect Joe Biden clears 270-vote thre...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37919 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id  parent_id    post_id  is_submitter  \\\n",
       "0        dpep775  t3_7b0y34  t3_7b0y34         False   \n",
       "1        dpee371  t3_7b0y34  t3_7b0y34         False   \n",
       "2        dpex3x5  t3_7b0y34  t3_7b0y34         False   \n",
       "3        dpejj86  t3_7b0y34  t3_7b0y34         False   \n",
       "4        dpeiwa9  t3_7b0y34  t3_7b0y34         False   \n",
       "...          ...        ...        ...           ...   \n",
       "37914    gfv68hn  t3_kd8bdc  t3_kd8bdc         False   \n",
       "37915    gfv78uj  t3_kd8bdc  t3_kd8bdc         False   \n",
       "37916    gfv7xr7  t3_kd8bdc  t3_kd8bdc         False   \n",
       "37917    gfv8r3z  t3_kd8bdc  t3_kd8bdc         False   \n",
       "37918    gfv9npw  t3_kd8bdc  t3_kd8bdc         False   \n",
       "\n",
       "                                                    body  score  stickied  \\\n",
       "0                  I love how onion updates to The city    1349     False   \n",
       "1                                              ...again.   4780     False   \n",
       "2      Just need to finish it off with \"Our thoughts ...    262     False   \n",
       "3      thousands of people going to prison for weed E...   2265     False   \n",
       "4      Every law abiding citizen in Australia that wa...   1958     False   \n",
       "...                                                  ...    ...       ...   \n",
       "37914  I predict this confirmation thing is going to ...      3     False   \n",
       "37915   I'm excited for the cheeto's rage tweeting later      3     False   \n",
       "37916  I wonder if Republicans will purposely not ref...      3     False   \n",
       "37917  Looking forward to when the whitehouse finally...      3     False   \n",
       "37918   And the world takes a collective sigh of relief.      3     False   \n",
       "\n",
       "               created_utc                                         post_title  \\\n",
       "0      2017-11-06 02:49:19  'No Way To Prevent This,’ Says Only Nation Whe...   \n",
       "1      2017-11-05 23:14:31  'No Way To Prevent This,’ Says Only Nation Whe...   \n",
       "2      2017-11-06 05:53:10  'No Way To Prevent This,’ Says Only Nation Whe...   \n",
       "3      2017-11-06 00:57:39  'No Way To Prevent This,’ Says Only Nation Whe...   \n",
       "4      2017-11-06 00:44:49  'No Way To Prevent This,’ Says Only Nation Whe...   \n",
       "...                    ...                                                ...   \n",
       "37914  2020-12-14 23:40:41  President-elect Joe Biden clears 270-vote thre...   \n",
       "37915  2020-12-14 23:49:44  President-elect Joe Biden clears 270-vote thre...   \n",
       "37916  2020-12-14 23:56:06  President-elect Joe Biden clears 270-vote thre...   \n",
       "37917  2020-12-15 00:03:35  President-elect Joe Biden clears 270-vote thre...   \n",
       "37918  2020-12-15 00:12:01  President-elect Joe Biden clears 270-vote thre...   \n",
       "\n",
       "      subreddit  \n",
       "0      TheOnion  \n",
       "1      TheOnion  \n",
       "2      TheOnion  \n",
       "3      TheOnion  \n",
       "4      TheOnion  \n",
       "...         ...  \n",
       "37914      news  \n",
       "37915      news  \n",
       "37916      news  \n",
       "37917      news  \n",
       "37918      news  \n",
       "\n",
       "[37919 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list =[onion_df, news_df]\n",
    "comments_df = pd.concat(df_list)\n",
    "comments_df.reset_index(drop=True, inplace=True)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> check if any row with missing data. \n",
    "\n",
    "> Outcome - no missing data, no data row dropped is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_id      0\n",
       "parent_id       0\n",
       "post_id         0\n",
       "is_submitter    0\n",
       "body            0\n",
       "score           0\n",
       "stickied        0\n",
       "created_utc     0\n",
       "post_title      0\n",
       "subreddit       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The columns not relevant to EDA and modeling will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = comments_df.drop(columns=['is_submitter','stickied','created_utc'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Invoke the \"text_cleaning\" function to perform text cleaning activities, and output the cleaned comments to \"body_cleaned\" new column . Refer to the comments on \"text_cleaning\" function for the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body_cleaned'] = comments_df['body'].map(text_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After text cleaning, check if any row with missing data\n",
    "\n",
    "> Outcome - no missing data, no data row dropped is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment_id      0\n",
       "parent_id       0\n",
       "post_id         0\n",
       "body            0\n",
       "score           0\n",
       "post_title      0\n",
       "subreddit       0\n",
       "body_cleaned    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Invoke the \"lematization\" function to perform lemmatization, and output the lemmatized comments to \"body_cleaned_lemmatized\" new column . Refer to the comments on \"sentence_lemmatizer\" function for the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['body_cleaned_lemmatized'] = comments_df['body_cleaned'].map(sentence_lemmatizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lemmatization function has explicitly set \"np.NaN\" when there is blank comment text. This is to ensure the empty comment text can be identified by isnull() and dropped as part of data cleaning actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df['body_cleaned_lemmatized'].isnull().sum() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = comments_df.dropna(subset=['body_cleaned_lemmatized'])\n",
    "comments_df['body_cleaned_lemmatized'].isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quick verification the new columns were created with cleaned and lemmatized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>post_title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>body_cleaned</th>\n",
       "      <th>body_cleaned_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dpep775</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>I love how onion updates to The city</td>\n",
       "      <td>1349</td>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>love onion updates city</td>\n",
       "      <td>love onion update city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dpee371</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>...again.</td>\n",
       "      <td>4780</td>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>again</td>\n",
       "      <td>again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dpex3x5</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>t3_7b0y34</td>\n",
       "      <td>Just need to finish it off with \"Our thoughts ...</td>\n",
       "      <td>262</td>\n",
       "      <td>'No Way To Prevent This,’ Says Only Nation Whe...</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>need finish Our thoughts prayers go victims fa...</td>\n",
       "      <td>need finish our thought prayer go victim famil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id  parent_id    post_id  \\\n",
       "0    dpep775  t3_7b0y34  t3_7b0y34   \n",
       "1    dpee371  t3_7b0y34  t3_7b0y34   \n",
       "2    dpex3x5  t3_7b0y34  t3_7b0y34   \n",
       "\n",
       "                                                body  score  \\\n",
       "0              I love how onion updates to The city    1349   \n",
       "1                                          ...again.   4780   \n",
       "2  Just need to finish it off with \"Our thoughts ...    262   \n",
       "\n",
       "                                          post_title subreddit  \\\n",
       "0  'No Way To Prevent This,’ Says Only Nation Whe...  TheOnion   \n",
       "1  'No Way To Prevent This,’ Says Only Nation Whe...  TheOnion   \n",
       "2  'No Way To Prevent This,’ Says Only Nation Whe...  TheOnion   \n",
       "\n",
       "                                        body_cleaned  \\\n",
       "0                            love onion updates city   \n",
       "1                                              again   \n",
       "2  need finish Our thoughts prayers go victims fa...   \n",
       "\n",
       "                             body_cleaned_lemmatized  \n",
       "0                             love onion update city  \n",
       "1                                              again  \n",
       "2  need finish our thought prayer go victim famil...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check datatype on the comment text column to make sure it is string (i.e. Object). No type conversion is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 37779 entries, 0 to 37918\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   comment_id               37779 non-null  object\n",
      " 1   parent_id                37779 non-null  object\n",
      " 2   post_id                  37779 non-null  object\n",
      " 3   body                     37779 non-null  object\n",
      " 4   score                    37779 non-null  int64 \n",
      " 5   post_title               37779 non-null  object\n",
      " 6   subreddit                37779 non-null  object\n",
      " 7   body_cleaned             37779 non-null  object\n",
      " 8   body_cleaned_lemmatized  37779 non-null  object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "comments_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Export the cleaned dataframe into CSV - for EDA and Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "comments_df.to_csv(f\"../data/02_cleaned_data.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
